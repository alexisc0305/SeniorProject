{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial: https://github.com/ashima0109/VGG-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 14:54:52.307191: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def imports():\n",
    "    '''\n",
    "    imports(): All of the necessary imports to run the code. Users must have\n",
    "    the most up-to-date versions of the packages/libraries in order to\n",
    "    successfully run the code.\n",
    "    '''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import models\n",
    "import tensorboard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import gradio as gr\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(width, height, depth):\n",
    "    '''\n",
    "    build(): Constructs the VGG model.\n",
    "    \\t:param width: width of input images\n",
    "    \\t:param height: height of input images\n",
    "    \\t:param depth: depth (i.e., number of color channels) of input images\n",
    "    \\t:return: constructed model\n",
    "    '''\n",
    "\n",
    "    # reduce the number of convolutional layers (1, 2, or 3 layers) --> too many Conv2D layers = overfitting (model is learning too much)\n",
    "    # specify the learning rate for l2 norm (increase the value of the l2 norm)\n",
    "    # increase the perceentage of dropout layer\n",
    "    # 20% for validation, 80% for training (or 30% validation, 70% training); separate dataset for testing\n",
    "    # neurons for Conv2D layers (e.g., 128 decreased to 64 or 32)\n",
    "\n",
    "    # initialize model, input shape, and channel dimension\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1  \n",
    "\n",
    "    # CONV -> RELU -> POOL layer set\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\", input_shape = inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # (CONV -> RELU) * 2 -> POOL layer set\n",
    "    model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # (CONV -> RELU) * 3 -> POOL layer set\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # FC -> RELU layer set\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    # softmax classifier\n",
    "    model.add(Dense(11, kernel_regularizer = 'l2'))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    '''\n",
    "    preprocessing(): Preprocesses images in hair dataset.\n",
    "    \\t:return: tuple containing features and labels for images in hair dataset\n",
    "    '''\n",
    "\n",
    "    # access hair dataset from directory\n",
    "    DIRECTORY = r'new_dataset'\n",
    "    CATEGORIES = ['1', '2A', '2B', '2C', '3A', '3B', '3C', '4A', '4B', '4C', 'no_hair']\n",
    "    ENCODINGS = {\n",
    "        '1': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2A': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2B': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2C': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '3A': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        '3B': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        '3C': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        '4A': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        '4B': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        '4C': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        'no_hair': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # convert images to numpy arrays, and add them to data array\n",
    "    for category in CATEGORIES:\n",
    "        folder = os.path.join(DIRECTORY, category)\n",
    "\n",
    "        for img in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, img)\n",
    "            if (\".DS_Store\" in img_path):\n",
    "                continue\n",
    "            img_arr = cv2.imread(img_path)\n",
    "            img_arr = cv2.resize(img_arr, (224, 224))\n",
    "            encoding = ENCODINGS.get(category)\n",
    "            data.append([img_arr, encoding])\n",
    "\n",
    "    # shuffle data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # separate features and labels\n",
    "    for features, labels in data:\n",
    "        X.append(features)\n",
    "        Y.append(labels)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # return all features and labels\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    '''\n",
    "    test(): Implements the testing phase of the CHC model.\n",
    "    \\t:param model: trained CHC model\n",
    "    '''\n",
    "\n",
    "    \"\"\"\n",
    "    TEST_DATASET = ''\n",
    "    for img in os.listdir(TEST_DATASET):\n",
    "        # convert image to numpy array\n",
    "        img_arr = cv2.imread(img)\n",
    "        img_arr = cv2.resize(img_arr, (224, 224))\n",
    "\n",
    "        # show the image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        # model makes prediction\n",
    "        prediction = model.predict(img.reshape(-1, 224, 224, 3))\n",
    "\n",
    "        # print model prediction\n",
    "        print(\"Prediction = \" + str(prediction))\n",
    "    \"\"\"\n",
    "    \n",
    "    model.save(r'v2_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate():\n",
    "    '''\n",
    "    train_and_validate(): Implements the training and validation phases of the CHC model.\n",
    "    '''\n",
    "\n",
    "    # receive features and labels after preprocessing\n",
    "    tup = preprocessing()\n",
    "    X = tup[0]\n",
    "    Y = tup[1]\n",
    "    \n",
    "    # split hair dataset into training, validating, and testing datasets\n",
    "    \"\"\"\n",
    "    x_remainder, x_test, y_remainder, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 42)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_remainder, y_remainder, test_size = 0.1, random_state = 42)\n",
    "    \"\"\"\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    # normalize datasets\n",
    "    x_train = x_train / 255.0\n",
    "    x_valid = x_valid / 255.0\n",
    "\n",
    "    # set up tensorboard\n",
    "    NAME = f'hair-type-prediction-{int(time.time())}' \n",
    "    tensorboard = TensorBoard(log_dir=f'logs\\\\{NAME}\\\\')\n",
    "    \n",
    "    # construct the model; implement training and validation\n",
    "    model = build(224, 224, 3)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    # model.fit(x_train, y_train, epochs = 50, batch_size = 20, validation_data = (x_valid, y_valid), callbacks = tensorboard)\n",
    "    model.fit(x_train, y_train, epochs = 1, batch_size = 20, validation_data = (x_valid, y_valid))\n",
    "\n",
    "    # transition to testing\n",
    "    test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 14:55:23.619794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 333ms/step\n"
     ]
    }
   ],
   "source": [
    "# needs to get updated\n",
    "def make_prediction(img):\n",
    "    # retrieve model\n",
    "    model = models.load_model(r'v2_base')\n",
    "    \n",
    "    # model makes prediction\n",
    "    prediction = model.predict(img.reshape(-1, 224, 224, 3))\n",
    "\n",
    "    ENCODINGS = {\n",
    "        '1': 0,\n",
    "        '2A': 1,\n",
    "        '2B': 2,\n",
    "        '2C': 3,\n",
    "        '3A': 4,\n",
    "        '3B': 5,\n",
    "        '3C': 6,\n",
    "        '4A': 7,\n",
    "        '4B': 8,\n",
    "        '4C': 9,\n",
    "        'no_hair': 10\n",
    "    }\n",
    "\n",
    "    top = (-1 * sys.maxsize) - 1\n",
    "    index = -1\n",
    "    for i in range(len(prediction[0])):\n",
    "        if prediction[0][i] > top:\n",
    "            top = prediction[0][i]\n",
    "            index = i\n",
    "    \n",
    "    for key in ENCODINGS:\n",
    "        if ENCODINGS[key] == index:\n",
    "            return key\n",
    "\n",
    "demo = gr.Interface(fn = make_prediction, inputs = gr.Image(shape=(224, 224)), outputs = gr.Label(num_top_classes = 1)).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentation():\n",
    "    print(imports.__doc__)\n",
    "    print(preprocessing.__doc__)\n",
    "    print(build.__doc__)\n",
    "    print(train_and_validate.__doc__)\n",
    "    print(test.__doc__)\n",
    "\n",
    "documentation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
